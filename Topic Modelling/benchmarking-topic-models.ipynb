{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import spacy\n",
    "import gensim\n",
    "import warnings\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "pyLDAvis.enable_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# Get the CNN/DailyMail dataset\n",
    "ds,info = tfds.load(\"cnn_dailymail\", split = \"test\", with_info = True, shuffle_files = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "dataset = []\n",
    "for batch in tfds.as_numpy(ds.batch(1)):\n",
    "    dataset.append(re.sub(r\"[^a-z]+\", ' ', batch['highlights'][0].decode(\"utf-8\").lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "11490"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 7
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "['nottingham',\n 'forest',\n 'are',\n 'close',\n 'to',\n 'extending',\n 'dougie',\n 'freedman',\n 'contract',\n 'the',\n 'forest',\n 'boss',\n 'took',\n 'over',\n 'from',\n 'former',\n 'manager',\n 'stuart',\n 'pearce',\n 'in',\n 'february',\n 'freedman',\n 'has',\n 'since',\n 'lead',\n 'the',\n 'club',\n 'to',\n 'ninth',\n 'in']"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 8
    }
   ],
   "source": [
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc = True)) \n",
    "\n",
    "data_words = list(sent_to_words(dataset))\n",
    "\n",
    "data_words[:1][0][:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "bigram = gensim.models.Phrases(data_words, min_count = 5, threshold = 100) # higher threshold fewer phrases.\n",
    "trigram = gensim.models.Phrases(bigram[data_words], threshold = 100)  \n",
    "\n",
    "# Faster way to get a sentence clubbed as a trigram/bigram\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "stop_words.extend(['from', 'subject', 're', 'edu', 'use'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def remove_stopwords(texts):\n",
    "    return [[word for word in gensim.utils.simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "def lemmatization(texts, allowed_postags = ['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "['close',\n 'extend',\n 'boss',\n 'take',\n 'former',\n 'manager',\n 'ninth',\n 'championship']"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 12
    }
   ],
   "source": [
    "data_words_nostops = remove_stopwords(data_words)\n",
    "\n",
    "data_words_bigrams = make_bigrams(data_words_nostops)\n",
    "\n",
    "# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "nlp = spacy.load(\"en_core_web_md\", disable = ['parser', 'ner'])\n",
    "\n",
    "data_lemmatized = lemmatization(data_words_bigrams)\n",
    "\n",
    "data_lemmatized[:1][0][:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "id2word = gensim.corpora.Dictionary(data_lemmatized)\n",
    "\n",
    "texts = data_lemmatized\n",
    "\n",
    "corpus = [id2word.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "lda = gensim.models.LdaMulticore(corpus = corpus,\n",
    "                                   id2word = id2word,\n",
    "                                   num_topics = 10, \n",
    "                                   random_state = 100,\n",
    "                                   chunksize = 100,\n",
    "                                   passes = 20,\n",
    "                                   per_word_topics = True,\n",
    "                                   alpha = 0.01,\n",
    "                                   eta = 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "tfidf = gensim.models.TfidfModel(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "lda_tfidf = gensim.models.LdaMulticore(corpus = tfidf[corpus],\n",
    "                                       id2word = id2word,\n",
    "                                       num_topics = 10, \n",
    "                                       random_state = 100,\n",
    "                                       chunksize = 100,\n",
    "                                       passes = 20,\n",
    "                                       per_word_topics = True,\n",
    "                                       alpha = 0.01,\n",
    "                                       eta = 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "w2v = gensim.models.Word2Vec(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def compute_ones(vector):\n",
    "    vector_nums = [i[1] for i in vector]\n",
    "    left = 1 - sum(vector_nums)\n",
    "    count = 0\n",
    "    for i in vector_nums:\n",
    "        if i == 0.0:\n",
    "            count += 1\n",
    "    if count == 0:\n",
    "        return vector\n",
    "    else:\n",
    "        left = left/count\n",
    "        for i in range(len(vector)):\n",
    "            if vector[i][1] == 0.0:\n",
    "                vector[i] = (vector[i][0], left)\n",
    "        return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "text": [
      "100%|██████████| 11490/11490 [00:01<00:00, 6697.97it/s]\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "corpus_w2v = []\n",
    "for text in tqdm(corpus):\n",
    "    corpus_sent = []\n",
    "    for couple in text:\n",
    "        if(id2word[couple[0]] in w2v.wv.vocab.keys()):\n",
    "            corpus_sent.append((couple[0], sum(w2v.wv[id2word[couple[0]]].tolist())/len(w2v.wv[id2word[couple[0]]].tolist())))\n",
    "        else:\n",
    "            corpus_sent.append((couple[0], 0.0))\n",
    "    corpus_w2v.append(compute_ones(corpus_sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "lda_w2v = gensim.models.LdaMulticore(corpus = corpus_w2v,\n",
    "                                 id2word = id2word,\n",
    "                                 num_topics = 10, \n",
    "                                   random_state = 100,\n",
    "                                   chunksize = 100,\n",
    "                                   passes = 20,\n",
    "                                   per_word_topics = True,\n",
    "                                   alpha = 0.01,\n",
    "                                   eta = 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "ft = gensim.models.fasttext.FastText(size = 100)\n",
    "\n",
    "ft.build_vocab(sentences = texts)\n",
    "\n",
    "ft.train(\n",
    "    sentences = texts, epochs = ft.epochs,\n",
    "    total_examples = len(texts)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "text": [
      "100%|██████████| 11490/11490 [00:03<00:00, 3517.73it/s]\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "corpus_ft = []\n",
    "for text in tqdm(corpus):\n",
    "    corpus_sent = []\n",
    "    for couple in text:\n",
    "        corpus_sent.append((couple[0], sum(ft.wv[id2word[couple[0]]].tolist())/len(ft.wv[id2word[couple[0]]].tolist())))\n",
    "    corpus_ft.append(corpus_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "lda_ft = gensim.models.LdaMulticore(corpus = corpus_ft,\n",
    "                                 id2word = id2word,\n",
    "                                 num_topics = 10, \n",
    "                                   random_state = 100,\n",
    "                                   chunksize = 100,\n",
    "                                   passes = 20,\n",
    "                                   per_word_topics = True,\n",
    "                                   alpha = 0.01,\n",
    "                                   eta = 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def compute_coherence(model):\n",
    "    coherence_model_lda = gensim.models.CoherenceModel(model = model, texts = data_lemmatized, dictionary = id2word, coherence = 'c_v')\n",
    "    coherence_lda = coherence_model_lda.get_coherence()\n",
    "    print('Coherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "models = [lda, lda_tfidf, lda_w2v, lda_ft]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Coherence Score:  0.44382140039495066\n",
      "Coherence Score:  0.6041055006988955\n",
      "Coherence Score:  0.7612216016138255\n",
      "Coherence Score:  0.7612216016138255\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "for model in models:\n",
    "    compute_coherence(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\users\\test\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\IPython\\core\\formatters.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    339\u001b[0m                 \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    340\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 341\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mprinter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    342\u001b[0m             \u001b[1;31m# Finally look for special method names\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    343\u001b[0m             \u001b[0mmethod\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_real_method\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprint_method\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\test\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pyLDAvis\\_display.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(data, kwds)\u001b[0m\n\u001b[0;32m    311\u001b[0m     \u001b[0mformatter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mip\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdisplay_formatter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformatters\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'text/html'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    312\u001b[0m     formatter.for_type(PreparedData,\n\u001b[1;32m--> 313\u001b[1;33m                        lambda data, kwds=kwargs: prepared_data_to_html(data, **kwds))\n\u001b[0m\u001b[0;32m    314\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    315\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\test\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pyLDAvis\\_display.py\u001b[0m in \u001b[0;36mprepared_data_to_html\u001b[1;34m(data, d3_url, ldavis_url, ldavis_css_url, template_type, visid, use_http)\u001b[0m\n\u001b[0;32m    176\u001b[0m                            \u001b[0md3_url\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0md3_url\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    177\u001b[0m                            \u001b[0mldavis_url\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mldavis_url\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 178\u001b[1;33m                            \u001b[0mvis_json\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_json\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    179\u001b[0m                            ldavis_css_url=ldavis_css_url)\n\u001b[0;32m    180\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\test\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pyLDAvis\\_prepare.py\u001b[0m in \u001b[0;36mto_json\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    415\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    416\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mto_json\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 417\u001b[1;33m        \u001b[1;32mreturn\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNumPyEncoder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\test\\appdata\\local\\programs\\python\\python37\\lib\\json\\__init__.py\u001b[0m in \u001b[0;36mdumps\u001b[1;34m(obj, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[0m\n\u001b[0;32m    236\u001b[0m         \u001b[0mcheck_circular\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcheck_circular\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_nan\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mallow_nan\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mindent\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    237\u001b[0m         \u001b[0mseparators\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mseparators\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdefault\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msort_keys\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msort_keys\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 238\u001b[1;33m         **kw).encode(obj)\n\u001b[0m\u001b[0;32m    239\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    240\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\test\\appdata\\local\\programs\\python\\python37\\lib\\json\\encoder.py\u001b[0m in \u001b[0;36mencode\u001b[1;34m(self, o)\u001b[0m\n\u001b[0;32m    197\u001b[0m         \u001b[1;31m# exceptions aren't as detailed.  The list call should be roughly\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m         \u001b[1;31m# equivalent to the PySequence_Fast that ''.join() would do.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m         \u001b[0mchunks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miterencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_one_shot\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m             \u001b[0mchunks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\test\\appdata\\local\\programs\\python\\python37\\lib\\json\\encoder.py\u001b[0m in \u001b[0;36miterencode\u001b[1;34m(self, o, _one_shot)\u001b[0m\n\u001b[0;32m    255\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkey_separator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem_separator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort_keys\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    256\u001b[0m                 self.skipkeys, _one_shot)\n\u001b[1;32m--> 257\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_iterencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    258\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    259\u001b[0m def _make_iterencode(markers, _default, _encoder, _indent, _floatstr,\n",
      "\u001b[1;32mc:\\users\\test\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pyLDAvis\\utils.py\u001b[0m in \u001b[0;36mdefault\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    144\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    145\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 146\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mJSONEncoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdefault\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\test\\appdata\\local\\programs\\python\\python37\\lib\\json\\encoder.py\u001b[0m in \u001b[0;36mdefault\u001b[1;34m(self, o)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    178\u001b[0m         \"\"\"\n\u001b[1;32m--> 179\u001b[1;33m         raise TypeError(f'Object of type {o.__class__.__name__} '\n\u001b[0m\u001b[0;32m    180\u001b[0m                         f'is not JSON serializable')\n\u001b[0;32m    181\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Object of type complex is not JSON serializable"
     ],
     "ename": "TypeError",
     "evalue": "Object of type complex is not JSON serializable",
     "output_type": "error"
    },
    {
     "data": {
      "text/plain": "PreparedData(topic_coordinates=                        x                           y  topics  cluster  \\\ntopic                                                                    \n2     -0.136132+0.000000j  5.529429e-07+0.000000e+00j       1        1   \n7      0.015088+0.000000j -9.376932e-05+0.000000e+00j       2        1   \n6      0.015235+0.000000j  6.780211e-04+0.000000e+00j       3        1   \n9      0.015111+0.000000j -8.241395e-05+0.000000e+00j       4        1   \n5      0.015118+0.000000j -8.342310e-05+0.000000e+00j       5        1   \n8      0.015118+0.000000j -8.342310e-05+0.000000e+00j       6        1   \n4      0.015118+0.000000j -8.343691e-05+0.000000e+00j       7        1   \n3      0.015110+0.000000j -8.526169e-05+0.000000e+00j       8        1   \n1      0.015118+0.000000j -8.342293e-05+0.000000e+00j       9        1   \n0      0.015118+0.000000j -8.342305e-05+0.000000e+00j      10        1   \n\n            Freq  \ntopic             \n2      99.543541  \n7       0.056903  \n6       0.056751  \n9       0.052449  \n5       0.050611  \n8       0.047950  \n4       0.047950  \n3       0.047950  \n1       0.047950  \n0       0.047950  , topic_info=               Term        Freq       Total Category  logprob  loglift\n488           match  198.000000  198.000000  Default  30.0000  30.0000\n161          charge  337.000000  337.000000  Default  29.0000  29.0000\n456             new  316.000000  316.000000  Default  28.0000  28.0000\n117            week  393.000000  393.000000  Default  27.0000  27.0000\n316           leave  393.000000  393.000000  Default  26.0000  26.0000\n...             ...         ...         ...      ...      ...      ...\n4994      satirical    0.007608    7.741898  Topic10  -9.6178   0.7176\n4993     xenophobic    0.007608    9.624314  Topic10  -9.6178   0.4999\n4992    immigration    0.007608   23.994873  Topic10  -9.6178  -0.4136\n5018  contraceptive    0.007608    7.093773  Topic10  -9.6178   0.8050\n5019          crook    0.007608    5.476428  Topic10  -9.6178   1.0638\n\n[581 rows x 6 columns], token_table=       Topic      Freq        Term\nterm                              \n4998       1  1.030744     academy\n5991       1  0.953228      accent\n3422       1  1.036754     agonise\n25         1  1.000206        also\n11960      1  1.025096   aluminium\n...      ...       ...         ...\n310        1  1.001117       would\n4993       1  1.039035  xenophobic\n74         1  0.999932        year\n174        1  0.999500    year_old\n6785       1  1.090044       zesty\n\n[187 rows x 3 columns], R=30, lambda_step=0.01, plot_opts={'xlab': 'PC1', 'ylab': 'PC2'}, topic_order=[3, 8, 7, 10, 6, 9, 5, 4, 2, 1])"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 27
    }
   ],
   "source": [
    "#pyLDAvis.gensim.prepare(lda_tfidf, corpus, id2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Topic: 0 \n",
      "Words: 0.000*\"wireless\" + 0.000*\"observatorie\" + 0.000*\"mhz\" + 0.000*\"methanol\" + 0.000*\"lawnmower\" + 0.000*\"infinitesimal\" + 0.000*\"band\" + 0.000*\"guide\" + 0.000*\"wave\" + 0.000*\"say\"\n",
      "Topic: 1 \n",
      "Words: 0.000*\"margherita\" + 0.000*\"buttery\" + 0.000*\"shiraz\" + 0.000*\"scotch\" + 0.000*\"zesty\" + 0.000*\"sauvignon\" + 0.000*\"riesling\" + 0.000*\"merlot\" + 0.000*\"pair\" + 0.000*\"sausage\"\n",
      "Topic: 2 \n",
      "Words: 0.004*\"say\" + 0.002*\"year\" + 0.002*\"last\" + 0.002*\"make\" + 0.002*\"take\" + 0.002*\"police\" + 0.002*\"man\" + 0.002*\"also\" + 0.002*\"find\" + 0.002*\"week\"\n",
      "Topic: 3 \n",
      "Words: 0.000*\"garter\" + 0.000*\"skillfully\" + 0.000*\"stockings\" + 0.000*\"sensuality\" + 0.000*\"intimacy\" + 0.000*\"ranch\" + 0.000*\"wellness\" + 0.000*\"offering\" + 0.000*\"glove\" + 0.000*\"discuss\"\n",
      "Topic: 4 \n",
      "Words: 0.000*\"spiced\" + 0.000*\"crumbly\" + 0.000*\"delicately\" + 0.000*\"latte\" + 0.000*\"pastry\" + 0.000*\"espresso\" + 0.000*\"pairing\" + 0.000*\"cheese\" + 0.000*\"flavour\" + 0.000*\"irish\"\n",
      "Topic: 5 \n",
      "Words: 0.000*\"glimpse\" + 0.000*\"red\" + 0.000*\"season\" + 0.000*\"squirrel\" + 0.000*\"say\" + 0.000*\"wildlife\" + 0.000*\"also\" + 0.000*\"player\" + 0.000*\"marine\" + 0.000*\"point\"\n",
      "Topic: 6 \n",
      "Words: 0.000*\"waistcoat\" + 0.000*\"sleeveless\" + 0.000*\"monochrome\" + 0.000*\"floral\" + 0.000*\"stripe\" + 0.000*\"brighten\" + 0.000*\"skirt\" + 0.000*\"accent\" + 0.000*\"orange\" + 0.000*\"vibrant\"\n",
      "Topic: 7 \n",
      "Words: 0.000*\"cosmologist\" + 0.000*\"mathematician\" + 0.000*\"hawk\" + 0.000*\"famed\" + 0.000*\"aluminium\" + 0.000*\"flexible\" + 0.000*\"sing\" + 0.000*\"battery\" + 0.000*\"positively\" + 0.000*\"ionic\"\n",
      "Topic: 8 \n",
      "Words: 0.000*\"zesty\" + 0.000*\"sauvignon\" + 0.000*\"scotch\" + 0.000*\"shiraz\" + 0.000*\"merlot\" + 0.000*\"margherita\" + 0.000*\"buttery\" + 0.000*\"riesling\" + 0.000*\"sausage\" + 0.000*\"pair\"\n",
      "Topic: 9 \n",
      "Words: 0.000*\"zesty\" + 0.000*\"sauvignon\" + 0.000*\"scotch\" + 0.000*\"shiraz\" + 0.000*\"merlot\" + 0.000*\"margherita\" + 0.000*\"buttery\" + 0.000*\"riesling\" + 0.000*\"pair\" + 0.000*\"sausage\"\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "for idx, topic in lda_tfidf.print_topics(-1):\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[6.65336047e-05, 6.65336047e-05, 6.65336047e-05, ...,\n        6.65336047e-05, 6.65336047e-05, 6.65336047e-05],\n       [6.65336047e-05, 6.65336047e-05, 6.65336047e-05, ...,\n        6.65336047e-05, 6.65336047e-05, 6.65336047e-05],\n       [1.00349379e-03, 4.36801871e-04, 6.62492530e-04, ...,\n        1.00680845e-04, 8.49404023e-05, 8.86333728e-05],\n       ...,\n       [6.64998443e-05, 6.64998443e-05, 6.64998443e-05, ...,\n        6.64998443e-05, 6.64998443e-05, 6.64998443e-05],\n       [6.65336047e-05, 6.65336047e-05, 6.65336047e-05, ...,\n        6.65336047e-05, 6.65336047e-05, 6.65336047e-05],\n       [6.65213447e-05, 6.65213447e-05, 6.65213447e-05, ...,\n        6.65213447e-05, 6.65213447e-05, 6.65213447e-05]], dtype=float32)"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 37
    }
   ],
   "source": [
    "lda_tfidf.get_topics()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}